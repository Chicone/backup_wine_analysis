<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>classification &#8212; Wine Analysis 0.1.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=d1102ebc" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=12dfc556" />
    <script src="../_static/documentation_options.js?v=01f34227"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <h1>Source code for classification</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">sklearn.discriminant_analysis</span> <span class="kn">import</span> <span class="n">LinearDiscriminantAnalysis</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span><span class="p">,</span> <span class="n">Perceptron</span><span class="p">,</span> <span class="n">RidgeClassifier</span><span class="p">,</span> <span class="n">PassiveAggressiveClassifier</span><span class="p">,</span> <span class="n">SGDClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span><span class="p">,</span> <span class="n">GradientBoostingClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="n">find_first_and_last_position</span><span class="p">,</span> <span class="n">normalize_dict</span><span class="p">,</span> <span class="n">normalize_data</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">re</span>

<div class="viewcode-block" id="Classifier">
<a class="viewcode-back" href="../classification.html#classification.Classifier">[docs]</a>
<span class="k">class</span> <span class="nc">Classifier</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A classifier class that wraps around various machine learning algorithms</span>
<span class="sd">    provided by scikit-learn. This class allows for easy switching between different classifiers</span>
<span class="sd">    and provides methods for training and evaluating the models using cross-validation or separate datasets.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    data : numpy.ndarray</span>
<span class="sd">        The input data to be used for training and evaluation.</span>
<span class="sd">    labels : numpy.ndarray</span>
<span class="sd">        The labels corresponding to the input data.</span>
<span class="sd">    classifier_type : str, optional</span>
<span class="sd">        The type of classifier to use. Default is &#39;LDA&#39;.</span>
<span class="sd">        Supported values:</span>
<span class="sd">        - &#39;LDA&#39;: Linear Discriminant Analysis</span>
<span class="sd">        - &#39;LR&#39;: Logistic Regression</span>
<span class="sd">        - &#39;RFC&#39;: Random Forest Classifier</span>
<span class="sd">        - &#39;PAC&#39;: Passive Aggressive Classifier</span>
<span class="sd">        - &#39;PER&#39;: Perceptron</span>
<span class="sd">        - &#39;RGC&#39;: Ridge Classifier</span>
<span class="sd">        - &#39;SGD&#39;: Stochastic Gradient Descent Classifier</span>
<span class="sd">        - &#39;SVM&#39;: Support Vector Machine</span>
<span class="sd">        - &#39;KNN&#39;: K-Nearest Neighbors</span>
<span class="sd">        - &#39;DTC&#39;: Decision Tree Classifier</span>
<span class="sd">        - &#39;GNB&#39;: Gaussian Naive Bayes</span>
<span class="sd">        - &#39;GBC&#39;: Gradient Boosting Classifier</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">classifier_type</span><span class="o">=</span><span class="s1">&#39;LDA&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_classifier</span><span class="p">(</span><span class="n">classifier_type</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_classifier</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classifier_type</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the classifier object based on the classifier type.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        classifier_type : str</span>
<span class="sd">            The type of classifier to initialize. Supported types include &#39;LDA&#39;, &#39;LR&#39;, &#39;RFC&#39;,</span>
<span class="sd">            &#39;PAC&#39;, &#39;PER&#39;, &#39;RGC&#39;, &#39;SGD&#39;, &#39;SVM&#39;, &#39;KNN&#39;, &#39;DTC&#39;, &#39;GNB&#39;, and &#39;GBC&#39;.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        sklearn.base.BaseEstimator</span>
<span class="sd">            An instance of the selected scikit-learn classifier.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">classifier_type</span> <span class="o">==</span> <span class="s1">&#39;LDA&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">LinearDiscriminantAnalysis</span><span class="p">()</span>
        <span class="k">elif</span> <span class="n">classifier_type</span> <span class="o">==</span> <span class="s1">&#39;LR&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">classifier_type</span> <span class="o">==</span> <span class="s1">&#39;RFC&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">classifier_type</span> <span class="o">==</span> <span class="s1">&#39;PAC&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">PassiveAggressiveClassifier</span><span class="p">()</span>
        <span class="k">elif</span> <span class="n">classifier_type</span> <span class="o">==</span> <span class="s1">&#39;PER&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">Perceptron</span><span class="p">()</span>
        <span class="k">elif</span> <span class="n">classifier_type</span> <span class="o">==</span> <span class="s1">&#39;RGC&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">RidgeClassifier</span><span class="p">()</span>
        <span class="k">elif</span> <span class="n">classifier_type</span> <span class="o">==</span> <span class="s1">&#39;SGD&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">SGDClassifier</span><span class="p">()</span>
        <span class="k">elif</span> <span class="n">classifier_type</span> <span class="o">==</span> <span class="s1">&#39;SVM&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">classifier_type</span> <span class="o">==</span> <span class="s1">&#39;KNN&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">classifier_type</span> <span class="o">==</span> <span class="s1">&#39;DTC&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>
        <span class="k">elif</span> <span class="n">classifier_type</span> <span class="o">==</span> <span class="s1">&#39;GNB&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">GaussianNB</span><span class="p">()</span>
        <span class="k">elif</span> <span class="n">classifier_type</span> <span class="o">==</span> <span class="s1">&#39;GBC&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<div class="viewcode-block" id="Classifier.train_and_evaluate">
<a class="viewcode-back" href="../classification.html#classification.Classifier.train_and_evaluate">[docs]</a>
    <span class="k">def</span> <span class="nf">train_and_evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_splits</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">vintage</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">scaler_type</span><span class="o">=</span><span class="s1">&#39;standard&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Train and evaluate the classifier using cross-validation.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        n_splits : int, optional</span>
<span class="sd">            The number of splits for cross-validation. Default is 50.</span>
<span class="sd">        vintage : bool, optional</span>
<span class="sd">            Whether to process labels for vintage data. Default is False.</span>
<span class="sd">        random_seed : int, optional</span>
<span class="sd">            The random seed for reproducibility. Default is 42.</span>
<span class="sd">        test_size : float, optional</span>
<span class="sd">            The proportion of the dataset to include in the test split. If None, only one sample</span>
<span class="sd">            is used for testing. Default is None.</span>
<span class="sd">        normalize : bool, optional</span>
<span class="sd">            Whether to normalize the data. Default is False.</span>
<span class="sd">        scaler_type : str, optional</span>
<span class="sd">            The type of scaler to use for normalization if `normalize` is True. Default is &#39;standard&#39;.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        float</span>
<span class="sd">            The mean accuracy score from cross-validation.</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        This function performs cross-validation on the classifier and prints the accuracy and its standard deviation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Set the random seed for reproducibility of the results</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">random_seed</span><span class="p">)</span>

        <span class="c1"># Initialize an empty list to store accuracy scores from each cross-validation split</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Split&#39;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Perform cross-validation over the specified number of splits</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_splits</span><span class="p">):</span>
            <span class="c1"># Split the data into training and testing sets based on the provided parameters</span>
            <span class="n">train_indices</span><span class="p">,</span> <span class="n">test_indices</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_data</span><span class="p">(</span>
                <span class="n">vintage</span><span class="o">=</span><span class="n">vintage</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">test_size</span>
            <span class="p">)</span>

            <span class="c1"># Normalize across samples (vertically) the training and testing data if normalization is enabled</span>
            <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
                <span class="n">X_train</span><span class="p">,</span> <span class="n">scaler</span> <span class="o">=</span> <span class="n">normalize_data</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">scaler</span><span class="o">=</span><span class="n">scaler_type</span><span class="p">)</span>  <span class="c1"># Fit the scaler on training data</span>
                <span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>  <span class="c1"># Transform the test data using the train data scaler</span>

            <span class="c1"># Train the classifier on the training data</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

            <span class="c1"># Print the current split number every 5 iterations to show progress</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span>

            <span class="c1"># Evaluate the classifier on the testing data and append the accuracy score to the list</span>
            <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>

        <span class="c1"># Print a new line after the loop completes</span>
        <span class="nb">print</span><span class="p">()</span>

        <span class="c1"># Convert the list of scores to a numpy array for easier statistical calculations</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>

        <span class="c1"># Print the mean accuracy and the standard deviation across the cross-validation splits</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[96m&quot;</span> <span class="o">+</span> <span class="s2">&quot;Accuracy: </span><span class="si">%0.3f</span><span class="s2"> (+/- </span><span class="si">%0.3f</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[0m&quot;</span><span class="p">)</span>

        <span class="c1"># Return the mean accuracy score as the final result</span>
        <span class="k">return</span> <span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span></div>


<div class="viewcode-block" id="Classifier.train_and_evaluate_separate_datasets">
<a class="viewcode-back" href="../classification.html#classification.Classifier.train_and_evaluate_separate_datasets">[docs]</a>
    <span class="k">def</span> <span class="nf">train_and_evaluate_separate_datasets</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">n_splits</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">vintage</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                             <span class="n">random_seed</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">scaler_type</span><span class="o">=</span><span class="s1">&#39;standard&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Train the classifier on the provided training dataset and evaluate its performance on the testing dataset</span>
<span class="sd">        using cross-validation.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X_train : numpy.ndarray</span>
<span class="sd">            Training data.</span>
<span class="sd">        y_train : numpy.ndarray</span>
<span class="sd">            Training labels.</span>
<span class="sd">        X_test : numpy.ndarray</span>
<span class="sd">            Testing data.</span>
<span class="sd">        y_test : numpy.ndarray</span>
<span class="sd">            Testing labels.</span>
<span class="sd">        n_splits : int, optional</span>
<span class="sd">            The number of splits for cross-validation. Default is 50.</span>
<span class="sd">        vintage : bool, optional</span>
<span class="sd">            Whether to process labels for vintage data. Default is False.</span>
<span class="sd">        random_seed : int, optional</span>
<span class="sd">            The random seed for reproducibility. Default is 42.</span>
<span class="sd">        normalize : bool, optional</span>
<span class="sd">            Whether to normalize the data. Default is True.</span>
<span class="sd">        scaler_type : str, optional</span>
<span class="sd">            The type of scaler to use for normalization if `normalize` is True. Default is &#39;standard&#39;.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        float</span>
<span class="sd">            The mean accuracy score from cross-validation.</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        This function trains the classifier on the training data and evaluates it on the testing data.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Set the random seed for reproducibility, ensuring that the data splits and other random processes are consistent</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">random_seed</span><span class="p">)</span>

        <span class="c1"># Normalize the training data if the normalize flag is set to True</span>
        <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
            <span class="n">X_train</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">normalize_data</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">scaler</span><span class="o">=</span><span class="n">scaler_type</span><span class="p">)</span>

        <span class="c1"># Train the classifier using the training data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

        <span class="c1"># Initialize a new Classifier instance for the testing data</span>
        <span class="n">test_cls</span> <span class="o">=</span> <span class="n">Classifier</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">classifier_type</span><span class="o">=</span><span class="s1">&#39;LDA&#39;</span><span class="p">)</span>

        <span class="c1"># Initialize a list to store the accuracy scores from each split</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Print &#39;Split&#39; to indicate the start of cross-validation, keeping the output on the same line</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Split&#39;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Perform cross-validation for the specified number of splits</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_splits</span><span class="p">):</span>
            <span class="c1"># Split the testing data into &quot;in&quot; and &quot;out&quot; samples for cross-validation</span>
            <span class="n">in_indices</span><span class="p">,</span> <span class="n">out_indices</span><span class="p">,</span> <span class="n">X_in</span><span class="p">,</span> <span class="n">X_out</span><span class="p">,</span> <span class="n">y_in</span><span class="p">,</span> <span class="n">y_out</span> <span class="o">=</span> <span class="n">test_cls</span><span class="o">.</span><span class="n">split_data</span><span class="p">(</span><span class="n">vintage</span><span class="o">=</span><span class="n">vintage</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

            <span class="c1"># Normalize the samples if normalization is enabled</span>
            <span class="k">if</span> <span class="n">normalize</span><span class="p">:</span>
                <span class="n">X_in</span><span class="p">,</span> <span class="n">scaler</span> <span class="o">=</span> <span class="n">normalize_data</span><span class="p">(</span><span class="n">X_in</span><span class="p">,</span> <span class="n">scaler</span><span class="o">=</span><span class="n">scaler_type</span><span class="p">)</span>
                <span class="c1"># Use the same scaler fitted on X_in to transform X_out to ensure consistent scaling and prevent data</span>
                <span class="c1"># leakage.</span>
                <span class="n">X_out</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_out</span><span class="p">)</span>

            <span class="c1"># Evaluate the classifier on the &quot;out&quot; sample and append the score to the list</span>
            <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_out</span><span class="p">,</span> <span class="n">y_out</span><span class="p">))</span>

            <span class="c1"># Print the current split number every 5 iterations to show progress</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="c1"># Convert the list of scores to a numpy array for easier statistical calculations</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>

        <span class="c1"># Print a new line after the loop completes</span>
        <span class="nb">print</span><span class="p">()</span>

        <span class="c1"># Print the mean accuracy and the standard deviation across the cross-validation splits</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[96m&quot;</span> <span class="o">+</span> <span class="s2">&quot;Accuracy: </span><span class="si">%0.3f</span><span class="s2"> (+/- </span><span class="si">%0.3f</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[0m&quot;</span><span class="p">)</span>

        <span class="c1"># Return the mean accuracy score as the final result</span>
        <span class="k">return</span> <span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span></div>


    <span class="k">def</span> <span class="nf">_process_labels</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vintage</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Process the labels to extract relevant parts based on whether the data is vintage or not.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        vintage : bool</span>
<span class="sd">            If True, the function processes labels to extract a substring starting from the first digit</span>
<span class="sd">            found in the label (assuming vintage data formatting). If False, it processes labels to</span>
<span class="sd">            extract a single character or digit before the first digit found.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        numpy.ndarray</span>
<span class="sd">            An array of processed labels.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">processed_labels</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Iterate over each label in the labels list</span>
        <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">:</span>
            <span class="c1"># Search for the first digit in the label</span>
            <span class="n">match</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;\d+&#39;</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">vintage</span><span class="p">:</span>
                <span class="c1"># If processing vintage data, extract the substring starting from the first digit</span>
                <span class="n">processed_labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">label</span><span class="p">[</span><span class="n">match</span><span class="o">.</span><span class="n">start</span><span class="p">():])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># If not vintage, extract the character before the first digit</span>
                <span class="k">if</span> <span class="n">label</span><span class="p">[</span><span class="n">match</span><span class="o">.</span><span class="n">start</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;_&#39;</span><span class="p">:</span>
                    <span class="c1"># If the character before the digit is an underscore, take the character before the underscore</span>
                    <span class="n">lb</span> <span class="o">=</span> <span class="n">label</span><span class="p">[</span><span class="n">match</span><span class="o">.</span><span class="n">start</span><span class="p">()</span> <span class="o">-</span> <span class="mi">2</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Otherwise, take the character directly before the first digit</span>
                    <span class="n">lb</span> <span class="o">=</span> <span class="n">label</span><span class="p">[</span><span class="n">match</span><span class="o">.</span><span class="n">start</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
                <span class="n">processed_labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lb</span><span class="p">)</span>

        <span class="c1"># Return the processed labels as a numpy array</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">processed_labels</span><span class="p">)</span>

<div class="viewcode-block" id="Classifier.split_data">
<a class="viewcode-back" href="../classification.html#classification.Classifier.split_data">[docs]</a>
    <span class="k">def</span> <span class="nf">split_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vintage</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Split the data into training and testing sets based on labels.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        vintage : bool, optional</span>
<span class="sd">            Whether to process labels for vintage data. Default is False.</span>
<span class="sd">        test_size : float, optional</span>
<span class="sd">            The proportion of the dataset to include in the test split. If None, only one sample</span>
<span class="sd">            per unique label is used for testing. Default is None.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        tuple</span>
<span class="sd">            A tuple containing the following elements:</span>

<span class="sd">            - train_indices : numpy.ndarray</span>
<span class="sd">                Indices of the training data samples.</span>

<span class="sd">            - test_indices : numpy.ndarray</span>
<span class="sd">                Indices of the testing data samples.</span>

<span class="sd">            - X_train : numpy.ndarray</span>
<span class="sd">                The training data.</span>

<span class="sd">            - X_test : numpy.ndarray</span>
<span class="sd">                The testing data.</span>

<span class="sd">            - y_train : numpy.ndarray</span>
<span class="sd">                The labels for the training data.</span>

<span class="sd">            - y_test : numpy.ndarray</span>
<span class="sd">                The labels for the testing data.</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        This function splits the dataset into training and testing sets by first processing the labels.</span>
<span class="sd">        The splitting is done in such a way that either one sample per unique label is reserved for testing</span>
<span class="sd">        (if test_size is None) or a specified proportion of samples per label is reserved for testing.</span>
<span class="sd">        The samples are randomly shuffled before splitting to ensure randomness in the selection.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Process the labels according to whether they are vintage or not</span>
        <span class="n">processed_labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_labels</span><span class="p">(</span><span class="n">vintage</span><span class="p">)</span>

        <span class="c1"># Initialize lists to store indices for training and testing samples</span>
        <span class="n">test_indices</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">train_indices</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Iterate over each unique label to perform stratified splitting</span>
        <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">processed_labels</span><span class="p">):</span>
            <span class="c1"># Find indices of all samples corresponding to the current label</span>
            <span class="n">label_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">processed_labels</span><span class="p">)</span> <span class="o">==</span> <span class="n">label</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

            <span class="c1"># Shuffle these indices to ensure randomness in splitting</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">label_indices</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">test_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># If test_size is not specified, select one sample per label for testing</span>
                <span class="n">test_indices</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">label_indices</span><span class="p">[:</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Take the first shuffled index for testing</span>
                <span class="n">train_indices</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">label_indices</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>  <span class="c1"># The rest is for training</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># If test_size is specified, calculate the split point based on the test_size proportion</span>
                <span class="n">split_point</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">label_indices</span><span class="p">)</span> <span class="o">*</span> <span class="n">test_size</span><span class="p">)</span>
                <span class="n">test_indices</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">label_indices</span><span class="p">[:</span><span class="n">split_point</span><span class="p">])</span>  <span class="c1"># The first part goes into testing</span>
                <span class="n">train_indices</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">label_indices</span><span class="p">[</span><span class="n">split_point</span><span class="p">:])</span>  <span class="c1"># The remaining is for training</span>

        <span class="n">test_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">test_indices</span><span class="p">)</span>
        <span class="n">train_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">train_indices</span><span class="p">)</span>

        <span class="c1"># Split the data and labels into training and testing sets based on the calculated indices</span>
        <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">train_indices</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">test_indices</span><span class="p">]</span>
        <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">processed_labels</span><span class="p">)[</span><span class="n">train_indices</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">processed_labels</span><span class="p">)[</span><span class="n">test_indices</span><span class="p">]</span>

        <span class="c1"># Return the indices, data, and labels for both training and testing sets</span>
        <span class="k">return</span> <span class="n">train_indices</span><span class="p">,</span> <span class="n">test_indices</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span></div>
</div>



<div class="viewcode-block" id="process_labels">
<a class="viewcode-back" href="../classification.html#classification.process_labels">[docs]</a>
<span class="k">def</span> <span class="nf">process_labels</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">vintage</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Process a list of labels to extract relevant parts based on whether the data is vintage or not.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    labels : list of str</span>
<span class="sd">        A list of label strings to be processed.</span>
<span class="sd">    vintage : bool</span>
<span class="sd">        If True, the function processes labels to extract a substring starting from the first digit</span>
<span class="sd">        found in each label (assuming vintage data formatting). If False, it processes labels to</span>
<span class="sd">        extract a single character or digit before the first digit found.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    numpy.ndarray</span>
<span class="sd">        An array of processed labels.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    This function is similar to the `_process_labels` method within the `Classifier` class, but</span>
<span class="sd">    it operates on an external list of labels rather than an instance attribute.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">processed_labels</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Iterate over each label in the provided list of labels</span>
    <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">:</span>
        <span class="c1"># Search for the first digit in the label</span>
        <span class="n">match</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;\d+&#39;</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">vintage</span><span class="p">:</span>
            <span class="c1"># If processing vintage data, extract the substring starting from the first digit</span>
            <span class="n">processed_labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">label</span><span class="p">[</span><span class="n">match</span><span class="o">.</span><span class="n">start</span><span class="p">():])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># If not vintage, extract the character before the first digit</span>
            <span class="k">if</span> <span class="n">label</span><span class="p">[</span><span class="n">match</span><span class="o">.</span><span class="n">start</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;_&#39;</span><span class="p">:</span>
                <span class="c1"># If the character before the digit is an underscore, take the character before the underscore</span>
                <span class="n">lb</span> <span class="o">=</span> <span class="n">label</span><span class="p">[</span><span class="n">match</span><span class="o">.</span><span class="n">start</span><span class="p">()</span> <span class="o">-</span> <span class="mi">2</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Otherwise, take the character directly before the first digit</span>
                <span class="n">lb</span> <span class="o">=</span> <span class="n">label</span><span class="p">[</span><span class="n">match</span><span class="o">.</span><span class="n">start</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
            <span class="n">processed_labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lb</span><span class="p">)</span>

    <span class="c1"># Return the processed labels as a numpy array</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">processed_labels</span><span class="p">)</span></div>

</pre></div>

          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">Wine Analysis</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../main.html">Main script</a></li>
<li class="toctree-l1"><a class="reference internal" href="../wine_analysis.html">Wine analysis module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils.html">Utils module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dimensionality_reduction.html">Dimensionality reduction module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data_loader.html">Data loader module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visualizer.html">Visualizer module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../classification.html">Classification module</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  <li><a href="index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2024, Luis G. Camara.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 7.4.7</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 0.7.16</a>
      
    </div>

    

    
  </body>
</html>